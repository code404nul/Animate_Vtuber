import ollama
from utils.llm.memory_manager import MemoryManager

class OllamaChat:
    def __init__(self, model_name: str, max_turns: int = 20):
        """
        Initialise le chat avec un modÃ¨le Ollama
        
        Args:
            model_name: Nom du modÃ¨le dans Ollama
            max_turns: Nombre maximum de tours Ã  retenir en mÃ©moire
        """
        self.model_name = model_name
        self.memory = MemoryManager(max_turns=max_turns)
    
    def generate_response(self, user_message: str, stream: bool = True) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse avec le modÃ¨le
        
        Args:
            user_message: Message de l'utilisateur
            stream: Si True, affiche la rÃ©ponse en streaming
            
        Returns:
            RÃ©ponse gÃ©nÃ©rÃ©e par le modÃ¨le
        """
        # Ajoute le message utilisateur
        self.memory.add_message('user', user_message)
        
        # RÃ©cupÃ¨re le contexte (fenÃªtre glissante automatique)
        messages = self.memory.get_context()
        
        try:
            response_content = ""
            
            if stream:
                print(f"\nðŸ¤– Assistant: ", end="", flush=True)
                
                stream_response = ollama.chat(
                    model=self.model_name,
                    messages=messages,
                    stream=True
                )
                
                for chunk in stream_response:
                    content = chunk['message']['content']
                    print(content, end="", flush=True)
                    response_content += content
                
                print()
            else:
                response = ollama.chat(
                    model=self.model_name,
                    messages=messages,
                    stream=False
                )
                response_content = response['message']['content']
                print(f"\nðŸ¤– Assistant: {response_content}")
            
            # Ajoute la rÃ©ponse au contexte
            self.memory.add_message('assistant', response_content)
            
            return response_content
            
        except ollama.ResponseError as e:
            print(f"\nâŒ Erreur Ollama: {e}")
            return ""
        except Exception as e:
            print(f"\nâŒ Erreur inattendue: {e}")
            return ""
    
    def clear_memory(self):
        """Efface la mÃ©moire"""
        self.memory.clear()
        print("âœ¨ MÃ©moire effacÃ©e")
    
    def show_memory_info(self):
        """Affiche les informations sur la mÃ©moire"""
        print(f"\nðŸ“Š Ã‰tat de la mÃ©moire:")
        print(self.memory.get_summary())
    
    def show_context(self):
        """Affiche l'historique en mÃ©moire"""
        context = self.memory.get_context()
        print("\nðŸ“œ Contexte actuel en mÃ©moire:")
        for i, msg in enumerate(context, 1):
            role_emoji = "ðŸ‘¤" if msg['role'] == 'user' else "ðŸ¤–"
            preview = msg['content'][:80] + "..." if len(msg['content']) > 80 else msg['content']
            print(f"{i}. {role_emoji} {msg['role']}: {preview}")
